{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torchmetrics.retrieval import RetrievalMAP\nimport os\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport google.generativeai as genai\nimport os\nimport wandb\nimport ast\nimport subprocess\nimport pickle\nimport shutil\nimport datetime\nimport pytz\nfrom kaggle_secrets import UserSecretsClient\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:55:42.733404Z","iopub.execute_input":"2025-04-18T02:55:42.733792Z","iopub.status.idle":"2025-04-18T02:56:12.883659Z","shell.execute_reply.started":"2025-04-18T02:55:42.733758Z","shell.execute_reply":"2025-04-18T02:56:12.882881Z"}},"outputs":[{"name":"stderr","text":"2025-04-18 02:55:55.707705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744944955.904712      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744944955.957715      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# PositiveExampleMining","metadata":{}},{"cell_type":"code","source":"class MiningPosExample:\n    def __init__(self):\n        self.data = None  # Kh·ªüi t·∫°o thu·ªôc t√≠nh self.data l√† None\n\n     # Ph∆∞∆°ng th·ª©c __len__ ƒë·ªÉ tr·∫£ v·ªÅ s·ªë d√≤ng c·ªßa data\n    def __len__(self):\n        if self.data is not None:\n            return len(self.data)\n        return 0  # N·∫øu self.data ch∆∞a ƒë∆∞·ª£c g√°n (None), tr·∫£ v·ªÅ 0\n    \n    # Ph∆∞∆°ng th·ª©c __getitem__ ƒë·ªÉ truy xu·∫•t m·ªôt d√≤ng trong data theo ch·ªâ s·ªë\n    def __getitem__(self, index):\n        if self.data is not None:\n            return self.data.iloc[index]\n        raise IndexError(\"Index out of range\")  # N·∫øu self.data l√† None, raise l·ªói\n        \n    def mining_pos_example(self, data_file):\n        # ƒê·ªçc d·ªØ li·ªáu t·ª´ file (gi·∫£ s·ª≠ l√† file CSV)\n        df = pd.read_csv(data_file)\n        df['jobtitles'] = df['jobtitles'].apply(ast.literal_eval)\n        # L·∫•y c√°c c·ªôt 'jobtitles' v√† 'skills_gen', sau ƒë√≥ \"ph√¢n n·ªï\" danh s√°ch trong c·ªôt 'jobtitles'\n        new_df = df[['jobtitles', 'skills_gen']].explode('jobtitles').rename(columns={'jobtitles': 'jobtitle', 'skills_gen': 'skill'})\n        \n        # Th√™m c·ªôt 'label' v·ªõi gi√° tr·ªã to√†n b·ªô l√† 1\n        new_df['label'] = 1\n        \n        # L∆∞u k·∫øt qu·∫£ v√†o self.data\n        self.data = new_df\n        \n        return new_df\n\n    def get_data(self):\n        return self.data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:12.884729Z","iopub.execute_input":"2025-04-18T02:56:12.885052Z","iopub.status.idle":"2025-04-18T02:56:12.891197Z","shell.execute_reply.started":"2025-04-18T02:56:12.885032Z","shell.execute_reply":"2025-04-18T02:56:12.890365Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# HardNegativeExampleMining","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\nfrom datetime import datetime\n\nclass RetrievalTrainData:\n    def __init__(self, model_name, model_path=None):\n        self.model = BiEncoder(model_name).model\n        print(\"Load m√¥ h√¨nh.....\")\n        self.model_name = model_name\n        self.model_path = model_path        \n    \n    def prepare_query_corpus(self, data_file):\n        \"\"\"\n        Chu·∫©n b·ªã d·ªØ li·ªáu: chu·∫©n b·ªã c√°c corpus v√† queries cho t·ª´ng ng√¥n ng·ªØ.\n        \"\"\"\n        print(\"Chu·∫©n b·ªã data: ......\")\n        preparer = DataPreparer()\n        corpus_file = dict()\n        queries_file = dict()\n        langs = list(data_file['source_file'].keys())\n        print(langs)\n        for lang in langs:\n            print(f\"Chu·∫©n b·ªã data {lang}:.....\")\n            source_file = data_file['source_file'][lang]\n            corpus_file[lang], queries_file[lang] = preparer.prepare_query_corpus(source_file)\n        \n        return langs, corpus_file, queries_file\n\n    def inference(self, langs, corpus_file, queries_file):\n        \"\"\"\n        Th·ª±c hi·ªán inference cho t·ª´ng ng√¥n ng·ªØ.\n        \"\"\"\n        print(\"B·∫Øt ƒë·∫ßu inference.....\")\n        corpus, queries = dict(), dict()\n        for lang in langs:\n            print(f\"Load data {lang}:.....\")\n            corpus_file_cur = corpus_file[lang]\n            queries_file_cur = queries_file[lang]\n            corpus[lang], queries[lang] = Dataset.load_inference_data(corpus_file_cur, queries_file_cur)\n\n        inferencer = Inference(self.model)\n        for lang in langs:\n            print(f\"Inference {lang}:.....\")\n            corpus_cur, queries_cur = corpus[lang], queries[lang]\n            corpus_cur['embedding'], queries_cur['embedding'] = inferencer.embed(corpus_cur['jobtitle']), inferencer.embed(queries_cur['jobtitle'])\n\n        return corpus, queries, inferencer\n\n    def predict(self, langs, corpus, queries, inferencer):\n        \"\"\"\n        Th·ª±c hi·ªán d·ª± ƒëo√°n.\n        \"\"\"\n        print(\"B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n:.....\")\n        predictions = dict()\n        for lang in langs:\n            print(f\"D·ª± ƒëo√°n {lang}:.....\")\n            corpus_cur, queries_cur = corpus[lang], queries[lang] \n            predictions[lang] = inferencer.infer(corpus_cur, queries_cur)[:100]\n\n        return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:12.891918Z","iopub.execute_input":"2025-04-18T02:56:12.892182Z","iopub.status.idle":"2025-04-18T02:56:12.910649Z","shell.execute_reply.started":"2025-04-18T02:56:12.892163Z","shell.execute_reply":"2025-04-18T02:56:12.910044Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# DataPrepare","metadata":{}},{"cell_type":"code","source":"class DataPreparer:\n    def prepare_train_data(self, train_data_org_file):\n        print(\"ƒê·ªçc d·ªØ li·ªáu train_org:\")\n        \n        pos_miner = MiningPosExample()\n        pos_example_df = pos_miner.mining_pos_example(train_data_org_file)\n        print(pos_example_df.head())\n\n        train_df = pos_example_df\n        print(f\"C·ªôt d·ªØ li·ªáu: {train_df.columns}\")\n        print(\"Xu·∫•t d·ªØ li·ªáu train sau khi chu·∫©n b·ªã:\")\n        train_file = \"/kaggle/working/train.csv\"\n        utils.write_csv(train_df, train_file)\n        return train_df, train_file\n    \n    @staticmethod\n    def prepare_query_corpus(train_file):\n        train_data = pd.read_csv(train_file)\n        data = train_data[['tid', 'jobtitles']].rename(columns={'tid': 'g_id', 'jobtitles': 'jobtitle'})\n        data['jobtitle'] = data['jobtitle'].apply(ast.literal_eval)\n        data = data.explode('jobtitle').reset_index(drop=True)\n\n        query_df = data.reset_index(names='q_id')\n        corpus_df = data.reset_index(names='c_id')\n\n        print(\"Xu·∫•t query v√† corpus t·ª´ train:........\")\n        query_file = utils.write_csv(query_df, 'queries_train.csv')\n        corpus_file = utils.write_csv(corpus_df, 'corpus_train.csv')\n        return corpus_file, query_file \n\n    def prepare_inference_data(self, corpus_path, queries_path, lang):\n        print(\"ƒê·ªçc d·ªØ li·ªáu inference:\")\n        corpus_df = utils.read_tsv(corpus_path)\n        queries_df = utils.read_tsv(queries_path)\n        \n        print(\"Xu·∫•t d·ªØ li·ªáu inference:\")\n        corpus_out_path = f\"/kaggle/working/corpus_{lang}.csv\"\n        queries_out_path = f\"/kaggle/working/queries_{lang}.csv\"\n        utils.write_csv(corpus_df, corpus_out_path)\n        utils.write_csv(queries_df, queries_out_path)\n        return corpus_out_path, queries_out_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:12.912421Z","iopub.execute_input":"2025-04-18T02:56:12.912664Z","iopub.status.idle":"2025-04-18T02:56:12.929191Z","shell.execute_reply.started":"2025-04-18T02:56:12.912649Z","shell.execute_reply":"2025-04-18T02:56:12.928564Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class Dataset():\n    def __init__(self, data):\n        self.data = []\n        for jobtitle, skill, label in data:\n            self.data.append(InputExample(texts = [jobtitle, skill], label=label))\n\n    def __len__(self) -> int:\n        \"\"\"Tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng m·∫´u trong dataset.\"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n    @staticmethod\n    def load_train_data(train_path):\n        train_df = utils.read_csv(train_path)\n        jobtitles =  train_df['jobtitle'].tolist()\n        skills =  train_df['skill'].tolist()\n        labels = train_df['label'].tolist()\n\n        data = []\n        for idx, jobtitle in enumerate(jobtitles):\n            data.append((jobtitle, skills[idx], labels[idx]))\n        return data\n        \n    @staticmethod  \n    def load_inference_data(corpus_path, queries_path):\n        corpus_df = utils.read_csv(corpus_path)\n        queries_df = utils.read_csv(queries_path)\n        \n        cids_l = corpus_df['c_id'].tolist()\n        corpus_l = corpus_df['jobtitle'].tolist()\n        qids_l = queries_df['q_id'].tolist()\n        queries_l = queries_df['jobtitle'].tolist()\n\n        corpus = {\"c_id\": cids_l,\n                \"jobtitle\": corpus_l\n                }\n\n        queries = {\"q_id\": qids_l,\n                \"jobtitle\": queries_l\n                }\n        return corpus, queries","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:12.930004Z","iopub.execute_input":"2025-04-18T02:56:12.930233Z","iopub.status.idle":"2025-04-18T02:56:12.949643Z","shell.execute_reply.started":"2025-04-18T02:56:12.930219Z","shell.execute_reply":"2025-04-18T02:56:12.948861Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# BiEncoder","metadata":{}},{"cell_type":"code","source":"class BiEncoder:\n    def __init__(self, model_name=None, model_path=None):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(\"Device s·ª≠ d·ª•ng:\", device)\n        \n        try:\n            if model_path is None:\n                print(f\"T·∫£i m√¥ h√¨nh t·ª´ Hugging Face v·ªõi t√™n: {model_name}\")\n                self.model = SentenceTransformer(model_name)\n            else:\n                print(f\"T·∫£i m√¥ h√¨nh t·ª´ ƒë∆∞·ªùng d·∫´n c·ª•c b·ªô: {model_path}\")\n                self.model = SentenceTransformer(model_path)\n            \n            # ƒê·∫∑t m√¥ h√¨nh l√™n thi·∫øt b·ªã\n            self.model = self.model.to(device)\n            print(\"M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!\")\n        \n        except Exception as e:\n            print(f\"L·ªói khi kh·ªüi t·∫°o m√¥ h√¨nh: {e}\")\n            raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:12.950391Z","iopub.execute_input":"2025-04-18T02:56:12.950628Z","iopub.status.idle":"2025-04-18T02:56:12.968680Z","shell.execute_reply.started":"2025-04-18T02:56:12.950610Z","shell.execute_reply":"2025-04-18T02:56:12.967969Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# BiTrainer","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model_name, model_path=None):\n        self.bi_encoder = BiEncoder(model_name, model_path)\n\n    def train(self, dataset, loss, params):\n        print(\"Kh·ªüi t·∫°o dataset:\")\n        train_dataloader = DataLoader(dataset, shuffle=True, batch_size=16)\n\n        print(\"B·∫Øt ƒë·∫ßu train: \")\n\n         # Kh·ªüi t·∫°o h√†m m·∫•t m√°t\n        train_loss = loss(self.bi_encoder.model)\n        \n        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n        os.makedirs(params['output_path'], exist_ok=True)\n        \n        # Hu·∫•n luy·ªán v·ªõi callback\n        self.bi_encoder.model.fit(\n            train_objectives=[(train_dataloader, train_loss)],\n            epochs=params['num_epochs'],\n            warmup_steps=params['warmup_steps'],\n            output_path=params[\"output_path\"],\n            show_progress_bar=True\n        )\n\n        return self.bi_encoder.model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:12.969509Z","iopub.execute_input":"2025-04-18T02:56:12.969802Z","iopub.status.idle":"2025-04-18T02:56:12.989908Z","shell.execute_reply.started":"2025-04-18T02:56:12.969783Z","shell.execute_reply":"2025-04-18T02:56:12.989241Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"class Inference:\n    def __init__(self, model):\n        self.model = model\n\n    def embed(self, texts):\n        print(\"B·∫Øt ƒë·∫ßu ch·∫°y embeddings...\")\n        texts_embedding = self.model.encode(texts)\n        texts_embedding = torch.tensor(texts_embedding)\n\n        return texts_embedding\n\n    def infer(self, corpus, queries):    \n        class SimilarityModel(nn.Module):\n            def __init__(self, corpus_embeddings, corpus_cids):\n                super(SimilarityModel, self).__init__()\n                self.corpus_embeddings = corpus_embeddings  # 2D tensor of corpus embeddings\n                self.corpus_cids = corpus_cids              # List of CIDs\n        \n            def forward(self, question_embedding):\n                # Expand the question_embedding to match corpus_embeddings shape for cosine similarity calculation\n                question_embedding = question_embedding.unsqueeze(0).expand(self.corpus_embeddings.size(0), -1)\n                \n                # Compute cosine similarity\n                similarities = F.cosine_similarity(question_embedding, self.corpus_embeddings)\n                \n                # Get the top_n indices with the highest cosine similarity values\n                sorted_similarities, sorted_indices = torch.sort(similarities, descending=True)\n                \n                \n                # Return top_n_ids, sorted similarities, and sorted indices\n                return sorted_similarities, sorted_indices\n                \n        # Example device setup\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n\n        # Initialize the similarity model\n        corpus_embeddings = corpus[\"embedding\"].to(device)\n        cids = corpus['c_id']\n\n        query_embeddings = queries['embedding'].to(device)\n        qids = queries['q_id']\n        \n        similarity_model = SimilarityModel(corpus_embeddings, cids).to(device)\n        if torch.cuda.device_count() > 1:\n            similarity_model = nn.DataParallel(similarity_model)\n\n        self.predictions = []\n        for qid, query_embedding in tqdm(enumerate(query_embeddings), total=len(query_embeddings), desc=\"Processing queries\"):\n            # Convert question_embedding to tensor and move to the device\n            query_embedding = query_embedding.to(device)\n            \n            # Get the top_n most relevant CIDs\n            sorted_similarities, sorted_indices = similarity_model(query_embedding)\n            results = []\n            for idx in range(len(sorted_similarities)):\n                doc_id = sorted_indices[idx].item()\n                score = sorted_similarities[idx].item()\n                rank = idx\n                row = (qid + 1, \"Q0\", doc_id + 1, rank + 1, score, \"4Huiter\")\n                results.append(row)\n            self.predictions.append(results)\n        return self.predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:12.990666Z","iopub.execute_input":"2025-04-18T02:56:12.990906Z","iopub.status.idle":"2025-04-18T02:56:13.010957Z","shell.execute_reply.started":"2025-04-18T02:56:12.990886Z","shell.execute_reply":"2025-04-18T02:56:13.010100Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# RetrievalApp","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\nfrom datetime import datetime\n\nclass RetrievalApp:\n    def __init__(self, model_name, model_path=None):\n        self.model = BiEncoder(model_name).model\n        print(\"Load m√¥ h√¨nh.....\")\n        self.model_name = model_name\n        self.model_path = model_path\n    \n    def prepare_data(self, data_file):\n        \"\"\"\n        Chu·∫©n b·ªã d·ªØ li·ªáu: chu·∫©n b·ªã c√°c corpus v√† queries cho t·ª´ng ng√¥n ng·ªØ.\n        \"\"\"\n        print(\"Chu·∫©n b·ªã data: ......\")\n        preparer = DataPreparer()\n        corpus_file = dict()\n        queries_file = dict()\n        langs = list(data_file['corpus'].keys())\n        \n        for lang in langs:\n            print(f\"Chu·∫©n b·ªã data {lang}:.....\")\n            corpus_file_org = data_file['corpus'][lang]\n            queries_file_org = data_file['queries'][lang]\n            corpus_file[lang], queries_file[lang] = preparer.prepare_inference_data(corpus_file_org, queries_file_org, lang)\n        \n        return langs, corpus_file, queries_file\n\n    def inference(self, langs, corpus_file, queries_file):\n        \"\"\"\n        Th·ª±c hi·ªán inference cho t·ª´ng ng√¥n ng·ªØ.\n        \"\"\"\n        print(\"B·∫Øt ƒë·∫ßu inference.....\")\n        corpus, queries = dict(), dict()\n        for lang in langs:\n            print(f\"Load data {lang}:.....\")\n            corpus_file_cur = corpus_file[lang]\n            queries_file_cur = queries_file[lang]\n            corpus[lang], queries[lang] = Dataset.load_inference_data(corpus_file_cur, queries_file_cur)\n\n        inferencer = Inference(self.model)\n        for lang in langs:\n            print(f\"Inference {lang}:.....\")\n            corpus_cur, queries_cur = corpus[lang], queries[lang]\n            corpus_cur['embedding'], queries_cur['embedding'] = inferencer.embed(corpus_cur['jobtitle']), inferencer.embed(queries_cur['jobtitle'])\n\n        return corpus, queries, inferencer\n\n    def predict(self, langs, corpus, queries, inferencer):\n        \"\"\"\n        Th·ª±c hi·ªán d·ª± ƒëo√°n.\n        \"\"\"\n        print(\"B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n:.....\")\n        predictions = dict()\n        for lang in langs:\n            print(f\"D·ª± ƒëo√°n {lang}:.....\")\n            corpus_cur, queries_cur = corpus[lang], queries[lang] \n            predictions[lang] = inferencer.infer(corpus_cur, queries_cur)\n\n        return predictions\n\n    def zip_directory(self, zip_filename, dir_name):\n        \"\"\"\n        N√©n th∆∞ m·ª•c th√†nh file zip m√† kh√¥ng s·ª≠ d·ª•ng ƒëa lu·ªìng.\n        \"\"\"\n        print(f\"ƒêang n√©n th∆∞ m·ª•c {dir_name} th√†nh {zip_filename}...\")\n        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            # Duy·ªát qua t·∫•t c·∫£ c√°c file trong th∆∞ m·ª•c v√† n√©n ch√∫ng tu·∫ßn t·ª±\n            for root, dirs, files in os.walk(dir_name):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    arcname = os.path.relpath(file_path, dir_name)  # L∆∞u l·∫°i c·∫•u tr√∫c th∆∞ m·ª•c g·ªëc\n                    zipf.write(file_path, arcname)\n\n        print(f\"File zip ƒë√£ ƒë∆∞·ª£c t·∫°o: {zip_filename}\")\n\n    def save_predictions(self, langs, predictions):\n        \"\"\"\n        L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√†o file v√† n√©n th∆∞ m·ª•c.\n        \"\"\"\n        print(\"B·∫Øt ƒë·∫ßu xu·∫•t file:....\")\n        predictions_file = dict()\n        folder_name = f\"/kaggle/working/talent_clef/predict/{self.model_name}/{Timer.get()}\"\n        os.makedirs(folder_name, exist_ok=True)\n        for lang in langs:\n            predictions_file[lang] = utils.write_predictions(predictions[lang], folder_name, lang)\n        \n        # N√©n th∆∞ m·ª•c sau khi xu·∫•t file\n        zip_filename = folder_name + \".zip\"\n        self.zip_directory(zip_filename, folder_name)\n        \n        return predictions_file, zip_filename\n\n    def evaluate(self, langs, predictions_file, data):\n        \"\"\"\n        ƒê√°nh gi√° k·∫øt qu·∫£ d·ª± ƒëo√°n.\n        \"\"\"\n        print(\"B·∫Øt ƒë·∫ßu ƒë√°nh gi√°:.....\")\n        ratings = dict()\n        for lang in langs:\n            print(f\"ƒê√°nh gi√° {lang}:.....\")\n            run_file, qrels_file = predictions_file[lang], data['qrels'][lang]\n            ratings[lang] = Evaluate.evaluate(run_file, qrels_file)\n        return ratings\n\n    def __call__(self, data_file):\n        \"\"\"\n        N·ªëi c√°c h√†m l·∫°i v·ªõi nhau v√† ch·∫°y to√†n b·ªô quy tr√¨nh.\n        \"\"\"\n        langs, corpus_file, queries_file = self.prepare_data(data_file)\n        corpus, queries, inferencer = self.inference(langs, corpus_file, queries_file)\n        predictions = self.predict(langs, corpus, queries, inferencer)\n        predictions_file, zip_filename = self.save_predictions(langs, predictions)\n        ratings = self.evaluate(langs, predictions_file, data_file)\n        return ratings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:13.011725Z","iopub.execute_input":"2025-04-18T02:56:13.011894Z","iopub.status.idle":"2025-04-18T02:56:13.027394Z","shell.execute_reply.started":"2025-04-18T02:56:13.011882Z","shell.execute_reply":"2025-04-18T02:56:13.026705Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# SetupEnvironment","metadata":{}},{"cell_type":"code","source":"def load_git_workspace_wandb():\n    user_secrets = UserSecretsClient()\n    git_token = user_secrets.get_secret(\"github\")\n    wandp_api = user_secrets.get_secret(\"wandb\")\n\n    import subprocess\n\n    # Thay {git_token} b·∫±ng token th·ª±c t·∫ø c·ªßa b·∫°n\n    repo_url = f\"https://hoivd:{git_token}@github.com/hoivd/talent_clef\"\n    \n    # L·ªánh git clone\n    command = [\"git\", \"clone\", repo_url]\n    \n    try:\n        # Ch·∫°y l·ªánh v√† ƒë·ª£i ho√†n t·∫•t\n        result = subprocess.run(command, check=True, text=True, capture_output=True)\n        print(\"Clone th√†nh c√¥ng!\")\n        print(\"Stdout:\", result.stdout)  # In stdout n·∫øu c√≥\n        print(\"Stderr:\", result.stderr)  # In stderr ƒë·ªÉ th·∫•y ti·∫øn tr√¨nh\n    except subprocess.CalledProcessError as e:\n        print(\"L·ªói khi clone repository:\")\n        print(e.stderr)  # In th√¥ng b√°o l·ªói n·∫øu c√≥\n        # ƒêƒÉng nh·∫≠p W&B\n    \n    wandb.login(key=wandp_api)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:13.029492Z","iopub.execute_input":"2025-04-18T02:56:13.029684Z","iopub.status.idle":"2025-04-18T02:56:13.047754Z","shell.execute_reply.started":"2025-04-18T02:56:13.029670Z","shell.execute_reply":"2025-04-18T02:56:13.047171Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"class utils:\n    @staticmethod\n    def read_csv(input_path, columns=None):\n        print(\"ƒê·ªçc csv file:\")\n        # Ki·ªÉm tra input_path ngay t·ª´ ƒë·∫ßu\n        if input_path is None:\n            raise ValueError(\"input_path kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng (None). Vui l√≤ng cung c·∫•p ƒë∆∞·ªùng d·∫´n file CSV.\")\n        \n        try:  \n            if columns is None:\n                df = pd.read_csv(input_path, encoding='utf-8')\n            else:\n                df = pd.read_csv(input_path, encoding='utf-8', names=columns)\n            print(f\"ƒê·ªçc d·ªØ li·ªáu t·ª´ {input_path} th√†nh c√¥ng\")\n            return df\n        except Exception as e:\n            print(f\"L·ªói khi ƒë·ªçc d·ªØ li·ªáu t·ª´ {input_path}: {e}\")\n            raise\n\n\n    @staticmethod\n    def read_tsv(input_path, columns=None):\n        print(\"ƒê·ªçc tsv file:\")\n        # Ki·ªÉm tra input_path ngay t·ª´ ƒë·∫ßu\n        if input_path is None:\n            raise ValueError(\"input_path kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng (None). Vui l√≤ng cung c·∫•p ƒë∆∞·ªùng d·∫´n file TSV.\")\n        \n        try:  \n            df = None\n            if columns is None:\n                df = pd.read_csv(input_path, sep='\\t', encoding='utf-8')  # Th√™m sep='\\t' cho TSV\n            else:\n                df = pd.read_csv(input_path, sep='\\t', encoding='utf-8', names=columns)\n            \n            print(f\"ƒê·ªçc d·ªØ li·ªáu t·ª´ {input_path} th√†nh c√¥ng\")\n            print(df.head())\n            return df\n        except Exception as e:\n            print(f\"L·ªói khi ƒë·ªçc d·ªØ li·ªáu t·ª´ {input_path}: {e}\")\n            raise\n\n    @staticmethod\n    def write_csv(df, output_path):\n        try:\n            # Xu·∫•t ra file CSV\n            df.to_csv(output_path, sep=',', encoding='utf-8', index=False)\n            print(f\"ƒê√£ xu·∫•t d·ªØ li·ªáu ra {output_path}\")\n            return output_path\n        except Exception as e:\n            print(f\"L·ªói khi xu·∫•t file CSV: {e}\")\n            raise\n        \n    @staticmethod\n    def write_predictions(predictions, folder_name, lang):\n        \n        output_path = f\"{folder_name}/run_{lang}.trec\"\n        try:\n            with open(output_path, 'w', encoding='utf-8') as f:\n                for query_predict in predictions: \n                    for line in query_predict:  # rank b·∫Øt ƒë·∫ßu t·ª´ 1\n                        f.write(' '.join(str(x) for x in line) + '\\n')\n            print(f\"ƒê√£ xu·∫•t file TREC ra {output_path}\")  \n            return output_path\n        \n        except Exception as e:\n            print(f\"L·ªói khi xu·∫•t file TREC: {e}\")\n            raise\n\n        return output_path\n\n    @staticmethod\n    def write_pkl(data, output_path):\n        print(\"Ghi file pkl:........\")\n        with open(output_path, 'wb') as f:\n            pickle.dump(data, f)\n        print(\"Ghi file th√†nh c√¥ng\")\n        return output_path\n\n    @staticmethod\n    def read_pkl(output_path):\n        print(\"ƒê·ªçc file pkl:........\")\n        with open(output_path, 'rb') as f:\n            loaded_data = pickle.load(f)\n        print(\"ƒê·ªçc file pkl th√†nh c√¥ng\")\n        return loaded_data\n\n    @staticmethod\n    def split_dict_to_parts(data, num_parts=5):\n        # H√†m chia m·ªôt list th√†nh c√°c ph·∫ßn\n        def split_list(lst, num_parts):\n            avg_len = len(lst) // num_parts\n            parts = [lst[i * avg_len: (i + 1) * avg_len] for i in range(num_parts - 1)]\n            parts.append(lst[(num_parts - 1) * avg_len:])  # ph·∫ßn c√≤n l·∫°i\n            return parts\n        \n        dict_parts = []\n        \n        # Duy·ªát qua t·∫•t c·∫£ c√°c ng√¥n ng·ªØ trong data\n        for lang, values in data.items():\n            # Duy·ªát qua t·∫•t c·∫£ c√°c list trong m·ªói ng√¥n ng·ªØ\n            for key, value in values.items():\n                # Chia list th√†nh c√°c ph·∫ßn\n                parts = split_list(value, num_parts)\n                \n                # T·∫°o c√°c dict con cho t·ª´ng ph·∫ßn c·ªßa m·ªói list\n                for i in range(num_parts):\n                    if len(dict_parts) <= i:\n                        dict_parts.append(dict())  # N·∫øu ch∆∞a c√≥ ph·∫ßn th√¨ t·∫°o dict m·ªõi\n                    if dict_parts[i].get(lang) == None:\n                        dict_parts[i][lang] = dict()\n                    dict_parts[i][lang][f\"{key}\"] = parts[i]\n    \n        return dict_parts\n\n    @staticmethod\n    def get_dict_in_range(data, start, end):\n        result = {}\n        \n        for lang, values in data.items():\n            # T·∫°o m·ªôt dict con ch·ª©a c√°c ph·∫ßn t·ª≠ trong kho·∫£ng t·ª´ start ƒë·∫øn end\n            result[lang] = {}\n            for key, value in values.items():\n                # L·∫•y c√°c ph·∫ßn t·ª≠ trong kho·∫£ng (start, end) cho t·ª´ng list\n                result[lang][key] = value[start:end+1]\n        \n        return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:13.048562Z","iopub.execute_input":"2025-04-18T02:56:13.048794Z","iopub.status.idle":"2025-04-18T02:56:13.077072Z","shell.execute_reply.started":"2025-04-18T02:56:13.048776Z","shell.execute_reply":"2025-04-18T02:56:13.076270Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"class Evaluate:\n    @staticmethod\n    def evaluate(predictions_path, qrels_path):\n        command = [\"python\", \"/kaggle/working/talentclef25_evaluation_script/talentclef_evaluate.py\", \"--qrels\", qrels_path, \"--run\", predictions_path]\n        result = subprocess.run(command, capture_output=True, text=True)\n        print(result.stdout)\n\n        return Evaluate.extract_metrics(result)\n        \n    @staticmethod\n    def extract_metrics(result, language=\"en-en\"):\n        stdout = result.stdout\n        map_value = float(stdout.split(\"map: \")[1].split(\"\\n\")[0])\n        mrr = float(stdout.split(\"mrr: \")[1].split(\"\\n\")[0])\n        ndcg = float(stdout.split(\"ndcg: \")[1].split(\"\\n\")[0])\n        precision_5 = float(stdout.split(\"precision@5: \")[1].split(\"\\n\")[0])\n        precision_10 = float(stdout.split(\"precision@10: \")[1].split(\"\\n\")[0])\n        precision_100 = float(stdout.split(\"precision@100: \")[1].split(\"\\n\")[0])\n    \n        metrics = {\n            \"map\": map_value,\n            \"mrr\": mrr,\n            \"ndcg\": ndcg,\n            \"precision@5\": precision_5,\n            \"precision@10\": precision_10,\n            \"precision@100\": precision_100\n        }\n        return metrics     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:13.077748Z","iopub.execute_input":"2025-04-18T02:56:13.078005Z","iopub.status.idle":"2025-04-18T02:56:13.100336Z","shell.execute_reply.started":"2025-04-18T02:56:13.077985Z","shell.execute_reply":"2025-04-18T02:56:13.099636Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# ModelLogger","metadata":{}},{"cell_type":"code","source":"class ModelLogger:\n    def __init__(self, model_name, loss_function, num_epochs, metrics, notes=\"\", training_time=None,\n                 folder=\"/kaggle/working/talent_clef/results\", file_name=\"model_info.csv\"):\n        self.model_name = model_name\n        self.loss = loss_function\n        self.epochs = num_epochs\n        self.metrics = metrics\n        self.notes = notes\n        self.training_time = training_time\n        self.folder = folder\n        self.file_path = os.path.join(folder, file_name)\n    \n    def compute_average_map(self):\n        map_values = [lang['map'] for lang in self.metrics.values() if 'map' in lang]\n        return sum(map_values) / len(map_values) if map_values else None\n\n    def to_dict(self):\n        return {\n            \"model_name\": [self.model_name],\n            \"Avg result\": [self.compute_average_map()],\n            \"en-en result\": [self.metrics.get(\"en-en\", \"\")],\n            \"es-es result\": [self.metrics.get(\"es-es\", \"\")],\n            \"de-de result\": [self.metrics.get(\"de-de\", \"\")],\n            \"zh-zh result\": [self.metrics.get(\"zh-zh\", \"\")],\n            \"en-es result\": [self.metrics.get(\"en-es\", \"\")],\n            \"en-de result\": [self.metrics.get(\"en-de\", \"\")],\n            \"en-zh result\": [self.metrics.get(\"en-zh\", \"\")],\n            \"loss\": [self.loss],\n            \"epochs\": [self.epochs],\n            \"training_time (s)\": [self.training_time],\n            \"date\": [Timer.get()],\n            \"notes\": [self.notes]\n        }\n\n    def save(self):\n        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i\n        os.makedirs(self.folder, exist_ok=True)\n\n        # T·∫°o DataFrame t·ª´ dict\n        df_new = pd.DataFrame(self.to_dict())\n\n        if os.path.exists(self.file_path):\n            df_existing = pd.read_csv(self.file_path)\n            df_updated = pd.concat([df_existing, df_new], ignore_index=True)\n            df_updated.to_csv(self.file_path, index=False)\n            print(f\"‚úÖ ƒê√£ th√™m d·ªØ li·ªáu v√†o file: {self.file_path}\")\n        else:\n            df_new.to_csv(self.file_path, index=False)\n            print(f\"‚úÖ ƒê√£ t·∫°o file m·ªõi: {self.file_path}\")\n\n    def show_log(self):\n        if os.path.exists(self.file_path):\n            print(f\"\\nüìÑ N·ªôi dung file log:\")\n            log_df = utils.read_csv(self.file_path)\n            print(log_df)\n        else:\n            print(\"‚ö†Ô∏è Ch∆∞a c√≥ file log ƒë·ªÉ hi·ªÉn th·ªã.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:13.101083Z","iopub.execute_input":"2025-04-18T02:56:13.101323Z","iopub.status.idle":"2025-04-18T02:56:13.121539Z","shell.execute_reply.started":"2025-04-18T02:56:13.101305Z","shell.execute_reply":"2025-04-18T02:56:13.120788Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Timer","metadata":{}},{"cell_type":"code","source":"class Timer:\n    @staticmethod\n    def get():\n        # L·∫•y m√∫i gi·ªù Vi·ªát Nam (UTC+7)\n        vietnam_timezone = pytz.timezone('Asia/Ho_Chi_Minh')\n        \n        # L·∫•y th·ªùi gian hi·ªán t·∫°i ·ªü UTC\n        utc_now = datetime.now(pytz.utc)\n        \n        # Chuy·ªÉn th·ªùi gian UTC sang m√∫i gi·ªù Vi·ªát Nam\n        vietnam_time = utc_now.astimezone(vietnam_timezone)\n        \n        # Tr·∫£ v·ªÅ th·ªùi gian ƒë√£ ƒë·ªãnh d·∫°ng theo ki·ªÉu YYYY-MM-DD HH:MM:SS\n        return vietnam_time.strftime(\"%m-%d_%H-%M-%S\")\n\n# G·ªçi h√†m v√† in k·∫øt qu·∫£\nprint(Timer.get())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:13.122386Z","iopub.execute_input":"2025-04-18T02:56:13.122646Z","iopub.status.idle":"2025-04-18T02:56:13.173350Z","shell.execute_reply.started":"2025-04-18T02:56:13.122617Z","shell.execute_reply":"2025-04-18T02:56:13.172650Z"}},"outputs":[{"name":"stdout","text":"04-18_09-56-13\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# H√†m th·ª±c thi","metadata":{}},{"cell_type":"markdown","source":"## 1. Clone data","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhuggingface_api = user_secrets.get_secret(\"huggingface\")\n\n!huggingface-cli login --token {huggingface_api}\n!git clone https://huggingface.co/hoivinh20789/talent_clef /kaggle/working/models\nload_git_workspace_wandb()\n\n!git clone https://github.com/TalentCLEF/talentclef25_evaluation_script.git\n!pip install -r \"/kaggle/working/talentclef25_evaluation_script/requirements.txt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:56:48.401042Z","iopub.execute_input":"2025-04-18T02:56:48.401309Z","iopub.status.idle":"2025-04-18T02:57:27.235110Z","shell.execute_reply.started":"2025-04-18T02:56:48.401289Z","shell.execute_reply":"2025-04-18T02:57:27.234273Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `kaggle` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `kaggle`\nCloning into '/kaggle/working/models'...\nremote: Enumerating objects: 21, done.\u001b[K\nremote: Counting objects: 100% (18/18), done.\u001b[K\nremote: Compressing objects: 100% (16/16), done.\u001b[K\nremote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\nUnpacking objects: 100% (21/21), 76.29 KiB | 6.36 MiB/s, done.\nClone th√†nh c√¥ng!\nStdout: \nStderr: Cloning into 'talent_clef'...\nUpdating files:  61% (74/120)\nUpdating files:  62% (75/120)\nUpdating files:  63% (76/120)\nUpdating files:  64% (77/120)\nUpdating files:  65% (78/120)\nUpdating files:  66% (80/120)\nUpdating files:  67% (81/120)\nUpdating files:  68% (82/120)\nUpdating files:  69% (83/120)\nUpdating files:  70% (84/120)\nUpdating files:  71% (86/120)\nUpdating files:  72% (87/120)\nUpdating files:  73% (88/120)\nUpdating files:  74% (89/120)\nUpdating files:  75% (90/120)\nUpdating files:  76% (92/120)\nUpdating files:  77% (93/120)\nUpdating files:  78% (94/120)\nUpdating files:  79% (95/120)\nUpdating files:  80% (96/120)\nUpdating files:  80% (97/120)\nUpdating files:  81% (98/120)\nUpdating files:  82% (99/120)\nUpdating files:  83% (100/120)\nUpdating files:  84% (101/120)\nUpdating files:  85% (102/120)\nUpdating files:  86% (104/120)\nUpdating files:  87% (105/120)\nUpdating files:  88% (106/120)\nUpdating files:  89% (107/120)\nUpdating files:  90% (108/120)\nUpdating files:  91% (110/120)\nUpdating files:  92% (111/120)\nUpdating files:  93% (112/120)\nUpdating files:  94% (113/120)\nUpdating files:  95% (114/120)\nUpdating files:  96% (116/120)\nUpdating files:  97% (117/120)\nUpdating files:  98% (118/120)\nUpdating files:  99% (119/120)\nUpdating files: 100% (120/120)\nUpdating files: 100% (120/120), done.\n\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhoivinh20789\u001b[0m (\u001b[33mhoivinh20789-uit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Cloning into 'talentclef25_evaluation_script'...\nremote: Enumerating objects: 27, done.\u001b[K\nremote: Counting objects: 100% (27/27), done.\u001b[K\nremote: Compressing objects: 100% (25/25), done.\u001b[K\nremote: Total 27 (delta 10), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (27/27), 10.10 KiB | 10.10 MiB/s, done.\nResolving deltas: 100% (10/10), done.\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2.2.3)\nCollecting ranx (from -r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading ranx-0.3.20-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: numba>=0.54.1 in /usr/local/lib/python3.11/dist-packages (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (0.60.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (0.9.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (4.67.1)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (1.15.2)\nCollecting ir-datasets (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading ir_datasets-0.5.10-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (14.0.0)\nRequirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (3.10.15)\nCollecting lz4 (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting cbor2 (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading cbor2-5.6.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (0.12.2)\nCollecting fastparquet (from ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54.1->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (1.17.0)\nRequirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (2.9.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (2025.3.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (24.2)\nRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (4.13.3)\nCollecting inscriptis>=2.2.0 (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (5.3.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (6.0.2)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (2.32.3)\nCollecting trec-car-tools>=2.5.4 (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\nCollecting warc3-wet>=0.2.3 (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\nCollecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting zlib-state>=0.1.3 (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\nCollecting ijson>=3.1.3 (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nCollecting unlzw3>=0.2.1 (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (19.0.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (2.19.1)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from seaborn->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (3.7.5)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (4.13.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (0.1.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2)) (2025.1.31)\nCollecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets->ranx->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 2))\n  Downloading cbor-1.0.0.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas->-r /kaggle/working/talentclef25_evaluation_script/requirements.txt (line 1)) (2024.2.0)\nDownloading ranx-0.3.20-py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.3/99.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cbor2-5.6.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (249 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading ir_datasets-0.5.10-py3-none-any.whl (859 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\nDownloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\nDownloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\nDownloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\nBuilding wheels for collected packages: warc3-wet-clueweb09, cbor\n  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18920 sha256=bf98bfe79da9d74be98f04fe00bd65c5d3cd154c0b5127c0968f2d051b2a50bf\n  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53931 sha256=b89e554fa43dfd9d0155c0daa41ecaf7ef65a4cf86bf8c644495f2908670449e\n  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\nSuccessfully built warc3-wet-clueweb09 cbor\nInstalling collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, lz4, cbor2, inscriptis, trec-car-tools, ir-datasets, fastparquet, ranx\nSuccessfully installed cbor-1.0.0 cbor2-5.6.5 fastparquet-2024.11.0 ijson-3.3.0 inscriptis-2.6.0 ir-datasets-0.5.10 lz4-4.4.4 ranx-0.3.20 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.9\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# GenerateNegativePair","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\nmodel_path = \"/kaggle/working/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/epoch6\"\nretrieval = RetrievalTrainData(model_name, model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:57:55.570434Z","iopub.execute_input":"2025-04-18T02:57:55.571817Z","iopub.status.idle":"2025-04-18T02:58:04.828723Z","shell.execute_reply.started":"2025-04-18T02:57:55.571789Z","shell.execute_reply":"2025-04-18T02:58:04.827978Z"}},"outputs":[{"name":"stdout","text":"Device s·ª≠ d·ª•ng: cuda\nT·∫£i m√¥ h√¨nh t·ª´ Hugging Face v·ªõi t√™n: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"066a3c5377ff4bcc8cc82cdd31a4476d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d3fcf91fd44432a566bc5980aa5e2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31db5dd2559f463b8ae8e2d566271372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ca56e23020422c879832760b38ff52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"278419d874f745cd85bf21127875c391"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98068a25f2ec40e297c9702007ef8d42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61e0ec765fe241d19869bd0ffaa56745"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d976f435e2d4ba089fea0e80871d589"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"268ba276513c44eaa80e46ced2811faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f8c9af2965478cb544a3c032602245"}},"metadata":{}},{"name":"stdout","text":"M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!\nLoad m√¥ h√¨nh.....\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"data_file = {\n    \"source_file\": { \"en-en\": \"/kaggle/working/talent_clef/data/gen_data/train_data_gen.csv\"}\n}\nlangs, corpus_file, queries_file = retrieval.prepare_query_corpus(data_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:58:09.221003Z","iopub.execute_input":"2025-04-18T02:58:09.221306Z","iopub.status.idle":"2025-04-18T02:58:10.456731Z","shell.execute_reply.started":"2025-04-18T02:58:09.221285Z","shell.execute_reply":"2025-04-18T02:58:10.456057Z"}},"outputs":[{"name":"stdout","text":"Chu·∫©n b·ªã data: ......\n['en-en']\nChu·∫©n b·ªã data en-en:.....\nXu·∫•t query v√† corpus t·ª´ train:........\nƒê√£ xu·∫•t d·ªØ li·ªáu ra queries_train.csv\nƒê√£ xu·∫•t d·ªØ li·ªáu ra corpus_train.csv\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"corpus, queries, inferencer = retrieval.inference(langs, corpus_file, queries_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:58:13.015871Z","iopub.execute_input":"2025-04-18T02:58:13.016190Z","iopub.status.idle":"2025-04-18T02:58:29.593809Z","shell.execute_reply.started":"2025-04-18T02:58:13.016170Z","shell.execute_reply":"2025-04-18T02:58:29.592941Z"}},"outputs":[{"name":"stdout","text":"B·∫Øt ƒë·∫ßu inference.....\nLoad data en-en:.....\nƒê·ªçc csv file:\nƒê·ªçc d·ªØ li·ªáu t·ª´ corpus_train.csv th√†nh c√¥ng\nƒê·ªçc csv file:\nƒê·ªçc d·ªØ li·ªáu t·ª´ queries_train.csv th√†nh c√¥ng\nInference en-en:.....\nB·∫Øt ƒë·∫ßu ch·∫°y embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/844 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6176d6cac94b6a946bf7d763261f51"}},"metadata":{}},{"name":"stdout","text":"B·∫Øt ƒë·∫ßu ch·∫°y embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/844 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c77f2a2c608473ab6f5d5e2ca54ad7c"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"queries_parts = utils.split_dict_to_parts(queries, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:05:13.245099Z","iopub.execute_input":"2025-04-18T03:05:13.245443Z","iopub.status.idle":"2025-04-18T03:05:13.250180Z","shell.execute_reply.started":"2025-04-18T03:05:13.245423Z","shell.execute_reply":"2025-04-18T03:05:13.249445Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"part = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:05:18.868266Z","iopub.execute_input":"2025-04-18T03:05:18.868812Z","iopub.status.idle":"2025-04-18T03:05:18.872293Z","shell.execute_reply.started":"2025-04-18T03:05:18.868794Z","shell.execute_reply":"2025-04-18T03:05:18.871651Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"predictions = retrieval.predict(langs, corpus, queries_parts[part], inferencer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:05:20.944782Z","iopub.execute_input":"2025-04-18T03:05:20.945273Z","execution_failed":"2025-04-18T04:05:28.206Z"}},"outputs":[{"name":"stdout","text":"B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n:.....\nD·ª± ƒëo√°n en-en:.....\n","output_type":"stream"},{"name":"stderr","text":"Processing queries:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4559/9001 [54:11<53:20,  1.39it/s]  ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"output_path = \"/kaggle/working/negative_pair_new\"\nos.makedirs(output_path, exist_ok=True)\n\nprediction_path = f\"/kaggle/working/negative_pair_new/prediction_p{part+1}.pkl\"\nutils.write_pkl(predictions, prediction_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}